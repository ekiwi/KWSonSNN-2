{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Spotting on Spiking Neural Networks\n",
    "\n",
    "This is a test Jupyter notebook with implementations of various SNNs for keyword spotting (KWS) on SNNs. The networks developed will end up being implemented in an embedded hardware accelerator targeting low-power computing for e.g. hearing aids or similar. SNNs are ideal for such applications because of their sparse activity, and the application's relatively low requirements in terms of throughput. \n",
    "\n",
    "The notebook uses a combination of Numpy, PyTorch, and BindsNET libraries for this task. The dataset considered is a subset of relevant keywords from the speech commands dataset from Google, see [here](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html).\n",
    "\n",
    "The hardware accelerator is available in this repository under `./src/main/scala/neuroproc`. The accelerator is written in the Chisel language, which is an open-source HDL within Scala, see [here](https://github.com/chipsalliance/chisel3). Its design is based on [work by Anthon Riber](https://github.com/Thonner/NeuromorphicProcessor) and is developed as part of a master's thesis at the Institute of Mathematics and Computer Science, DTU Compute, Technical University of Denmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install bindsnet seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First step in this process is loading and understanding the dataset. Additionally, the data may need various kinds of preprocessing to be used in the models (e.g. FFT or similar, see [this](https://towardsdatascience.com/speech-classification-using-neural-networks-the-basics-e5b08d6928b7)). The box above should have downloaded the dataset - granted it is not already available.\n",
    "\n",
    "As part of loading the dataset, each of the signals is shortened to approximately half its original length by selection of the subsection with the largest absolute weighted sum. As an example, consider the following utterance of `'up'` and how a shifted Blackman window is used for weighting. Using a weighted sum rather than simply a sum allows for better control of where in the resulting snippet the word is placed. For now, the best results have been achieved with a centered weighting.\n",
    "\n",
    "First all of the considered frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch some audio\n",
    "sr, signal = wavfile.read('data/up/0a7c2a8d_nohash_0.wav')\n",
    "\n",
    "# Find some frames of it\n",
    "frame_length, frame_stride = 8000, 1000\n",
    "num_frames = (sr - frame_length) // frame_stride + 1\n",
    "indices = (\n",
    "    np.tile(np.arange(0, frame_length), (num_frames, 1))\n",
    "    + np.tile(\n",
    "        np.arange(0, num_frames * frame_stride, frame_stride), (frame_length, 1)\n",
    "    ).T\n",
    ")\n",
    "frames = signal[indices.astype(np.int32, copy=False)]\n",
    "\n",
    "# Plot all of the frames\n",
    "_, axs = plt.subplots(num_frames // 4 + 1, 4, figsize=(14,7), sharex=True, sharey=True)\n",
    "for i in range(4):\n",
    "    for j in range(min(4, num_frames - i*4)):\n",
    "        sns.lineplot(ax=axs[i, j], x=np.arange(frame_length), y=frames[i*4+j])\n",
    "        axs[i, j].set_xlim(0, frame_length)\n",
    "        axs[i, j].set_ylim(-32768, 32768)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "And now the actual windowing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the signal\n",
    "_, axs = plt.subplots(1, 3, figsize=(14,4))\n",
    "sns.lineplot(ax=axs[0], x=np.arange(len(signal)), y=signal)\n",
    "axs[0].set_xlim(0, len(signal))\n",
    "axs[0].set_ylim(-32768, 32768)\n",
    "axs[0].set_xlabel('Original signal')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the weigths\n",
    "weight = np.tile(np.blackman(int(frame_length*1.5))[-frame_length:], (num_frames, 1))\n",
    "sns.lineplot(ax=axs[1], x=np.arange(frame_length), y=np.blackman(frame_length))\n",
    "axs[1].set_xlim(0, frame_length)\n",
    "axs[1].set_ylim(0, 1.01)\n",
    "axs[1].set_xlabel('Weight function')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the one selected currently\n",
    "signal = frames[np.argmax(np.sum(np.dot(np.abs(frames), weight.T), 1))]\n",
    "sns.lineplot(ax=axs[2], x=np.arange(frame_length), y=signal)\n",
    "axs[2].set_xlim(0, frame_length)\n",
    "axs[2].set_ylim(-32768, 32768)\n",
    "axs[2].set_xlabel('Result signal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Let us load the remaining dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kwsonsnn.dataset import SpeechCommandsDataset\n",
    "kws = ['up', 'down', 'left', 'right', 'on', 'off', 'yes', 'no', 'go', 'stop']\n",
    "train_data = SpeechCommandsDataset('data', download=True,  preprocess=False, kws=kws)\n",
    "valid_data = SpeechCommandsDataset('data', download=False, preprocess=False, kws=kws, split='valid')\n",
    "test_data  = SpeechCommandsDataset('data', download=False, preprocess=False, kws=kws, split='test')"
   ]
  },
  {
   "source": [
    "Let us plot the distribution of keywords in the three splits."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kws = pd.DataFrame.from_dict({'Keyword' : kws, 'Count' : [0] * len(kws)}).set_index('Keyword')\n",
    "for datum in train_data:\n",
    "    train_kws['Count'][datum['label']] += 1\n",
    "valid_kws = pd.DataFrame.from_dict({'Keyword' : kws, 'Count' : [0] * len(kws)}).set_index('Keyword')\n",
    "for datum in valid_data:\n",
    "    valid_kws['Count'][datum['label']] += 1\n",
    "test_kws  = pd.DataFrame.from_dict({'Keyword' : kws, 'Count' : [0] * len(kws)}).set_index('Keyword')\n",
    "for datum in test_data:\n",
    "    test_kws['Count'][datum['label']] += 1\n",
    "_, axs = plt.subplots(1, 3, figsize=(14,4))\n",
    "sns.barplot(x=train_kws.index, y='Count', data=train_kws, ax=axs[0])\n",
    "sns.barplot(x=valid_kws.index, y='Count', data=valid_kws, ax=axs[1])\n",
    "sns.barplot(x=test_kws.index,  y='Count', data=test_kws,  ax=axs[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some random non-preprocessed data for each keyword. Unfortunately, there is no better way to fetch these than sequentially because of the data shuffling and use of sets in `SpeechCommandsDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwsS = set(kws)\n",
    "indices = []\n",
    "for i in range(len(train_data)):\n",
    "    if len(kwsS) == 0:\n",
    "        break\n",
    "    label = train_data[i]['label']\n",
    "    if label in kwsS:\n",
    "        indices.append(i)\n",
    "        kwsS.remove(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(indices) // 4 + 1\n",
    "cols = 4 if len(indices) >= 3 else len(indices)\n",
    "_, axs = plt.subplots(rows, cols, figsize=(14,7))\n",
    "for i in range(rows):\n",
    "    for j in range(min(cols, len(indices)-4*i)):\n",
    "        data = train_data[indices[i*4+j]]\n",
    "        audio, label = data['audio'], data['label']\n",
    "        sns.lineplot(ax=axs[i, j], x=np.arange(len(audio)), y=audio)\n",
    "        axs[i, j].set_xlim(0, len(audio))\n",
    "        axs[i, j].set_ylim(-1, 1)\n",
    "        axs[i, j].set_title(f'Keyword: {label}')\n",
    "plt.suptitle('Original audio signals')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "And now, consider four randomly selected different versions of the same keyword."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = kws[0]\n",
    "same = []\n",
    "for i in range(len(train_data)):\n",
    "    if len(same) == 4:\n",
    "        break\n",
    "    label = train_data[i]['label']\n",
    "    if kw == label:\n",
    "        same.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 4, figsize=(14,4))\n",
    "for i in range(4):\n",
    "    data = train_data[same[i]]\n",
    "    audio, label = data['audio'], data['label']\n",
    "    sns.lineplot(ax=axs[i], x=np.arange(len(audio)), y=audio)\n",
    "    axs[i].set_xlim(0, len(audio))\n",
    "    axs[i].set_ylim(-1, 1)\n",
    "plt.suptitle(f'Different versions of keyword \"{label}\"')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us apply the preprocessing steps we wish to do and plot the resulting data. First of all, an FFT of the data will effectively transform the data to a plottable spectrogram. Secondly, the frequency spectrum data will be converted into Mel cepstral coefficients.\n",
    "\n",
    "Various works consider a varying number of frames extracted from the audio signals. For example:\n",
    "- BindsNET's own spoken MNIST implementation uses `frame_length=25ms` and `frame_stride=10ms` (see [here](https://github.com/BindsNET/bindsnet/blob/master/bindsnet/datasets/spoken_mnist.py))\n",
    "- Hello Edge from Arm uses `frame_length=40ms` and `frame_stride=20ms` (see [here](https://arxiv.org/abs/1711.07128))\n",
    "- Benchmarking KWS Efficiency ... from Applied Brain Research uses 390-dimension frames with `frame_stride=10ms` (see [here](https://arxiv.org/abs/1812.01739))\n",
    "- Low-Power Low-Latency KWS ... from Arm, Intel Labs, Applied Brain Research etc. uses 390-dimension frames with `frame_stride=10ms` (see [here](https://arxiv.org/abs/2009.08921))\n",
    "- Max-Pooling Loss Training ... from Amazon and Google uses `frame_length=25ms` and `frame_stride=10ms` (see [here](https://arxiv.org/abs/1705.02411))\n",
    "- A Dataset and Taxonomy ... from NYU uses `frame_length=23.2ms` and `frame_stride=11.6ms` (see [here](https://dl.acm.org/doi/10.1145/2647868.2655045))\n",
    "- FastGRNN ... from Microsoft uses `frame_length=25ms` and `frame_stride=10ms` (see [here](https://arxiv.org/pdf/1901.02358.pdf))\n",
    "- Efficient KWS using Dilated ... from Snips uses `frame_length=25ms` and `frame_stride=10ms` (see [here](https://ieeexplore.ieee.org/document/8683474))\n",
    "- Deep Residual Learning ... uses `frame_length=30ms` and `frame_stride=10ms` (see [here](https://arxiv.org/abs/1710.10361))\n",
    "\n",
    "Overall, we see that a frame length of approximately 25ms with accordingly smaller stride (i.e., some amount of overlap) is typical for KWS applications. The overlap of frames serves to limit the risk of missing important information by poor frame \"placement\". Common to all these works is that they use significantly more frames than what is available to this project, if the network in the accelerator is not scaled. Currently, supporting only 22x22 input images limits is performance significantly as this means a significantly larger stride must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data._process_data()\n",
    "valid_data._process_data()\n",
    "test_data._process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us plot some processed data. We shall reuse the signals used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(rows, cols, figsize=(14,7))\n",
    "for i in range(rows):\n",
    "    for j in range(min(cols, len(indices)-4*i)):\n",
    "        data = train_data[indices[i*4+j]]\n",
    "        frames, label = data['audio'], data['label']\n",
    "        sns.heatmap(frames.T, ax=axs[i, j])\n",
    "        axs[i, j].invert_yaxis()\n",
    "        axs[i, j].set_title(f'Keyword: {kws[int(label.item())]}')\n",
    "plt.suptitle('Mel-spectrograms of audio signals')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "And returning back to the four versions of the same keyword."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 4, figsize=(14,4))\n",
    "for i in range(4):\n",
    "    data = train_data[same[i]]\n",
    "    frames, label = data['audio'], data['label']\n",
    "    sns.heatmap(frames.T, ax=axs[i])\n",
    "    axs[i].invert_yaxis()\n",
    "plt.suptitle(f'Different versions of keyword \"{kw}\"')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "After windowing and selecting only half of the original signal, the four different utterances of the same word have similar waveforms and spectrograms. Nonetheless, the network commonly confuses `'down'`, `'no'`, and `'go'`. Let us compare their respective spectrograms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skws = ['down', 'no', 'go']\n",
    "indices = list(map(lambda kw: kws.index(kw), skws))\n",
    "utterances = {k : [] for k in indices}\n",
    "for i in range(len(train_data)):\n",
    "    if all([len(v) == 4 for _, v in utterances.items()]):\n",
    "        break\n",
    "    label = train_data[i]['label'].long().item()\n",
    "    if label in indices and len(utterances[label]) < 4:\n",
    "        utterances[label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(3, 4, figsize=(14,7), sharex=True, sharey=True)\n",
    "for i, kw in enumerate(indices):\n",
    "    for j, ind in enumerate(utterances[kw]):\n",
    "        data = train_data[ind]\n",
    "        frames = data['audio']\n",
    "        sns.heatmap(frames.T, ax=axs[i, j], cbar=j==3)\n",
    "        axs[i, j].invert_yaxis()\n",
    "        if j==0:\n",
    "            axs[i, j].set_ylabel(kws[kw])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We see that, as expected, the spectrograms are rather similar - especially `'no'` and `'go'` show a lot of similarities, which makes sense considering how the only difference in their pronunciation is _n_ versus _g_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Encoding\n",
    "\n",
    "Next, we shall explore the encoding of the data fed into the network. Generally, we observe that power consumption is highly correlated with spike activity and thus, we should aim for as few spikes per time step as possible. The original work implements training and evaluation with rate-based encoding; direct rate encoding for simulation and training, and indirect rate-period encoding for efficient storage in the accelerator.\n",
    "\n",
    "This project, however, sets out to optimize this by using a timing-based encoding instead. This affects only the input to the network in that the activity in the subsequent layers is less dependent on data encoding.\n",
    "\n",
    "First, we encode the data with rate encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a Speech Commands example\n",
    "intensity = 32\n",
    "frames = train_data[0]['audio'] * intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct rate encoding\n",
    "from kwsonsnn.encode import RateEncoder\n",
    "enc = RateEncoder(500)\n",
    "print(enc(frames))\n",
    "print(enc(frames).sum())\n",
    "print(enc(mnist_frames))\n",
    "print(enc(mnist_frames).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect rate encoding\n",
    "from kwsonsnn.encode import RatePeriod\n",
    "enc = RatePeriod(500)\n",
    "print(enc(frames))\n",
    "print(enc(mnist_frames))"
   ]
  },
  {
   "source": [
    "Notice how the periods calculated above are typically rather short relative to the 500 time steps of inputting spikes. This means that a massive amount of spike activity is seen in the input phase, which will likely not be reflected in subsequent layers.\n",
    "\n",
    "Let us instead consider rank-order encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct rank-order encoding\n",
    "from kwsonsnn.encode import RankOrderDirect\n",
    "enc = RankOrderDirect(500)\n",
    "print(enc(frames))\n",
    "print(enc(frames).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect rank-order encoding\n",
    "from kwsonsnn.encode import RankOrderPeriod\n",
    "enc = RankOrderPeriod(500)\n",
    "print(enc(frames))"
   ]
  },
  {
   "source": [
    "When using rank-order encoding, at most one input neuron spikes in each of the 500 time steps. This means that the total number of spikes generated by input spike trains is reduced by roughly an order of magnitude with significant power savings to follow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The next step is to define a relevant model and prepare it for the supervised learning task that keyword spotting is. The following box checks that a pretrained model exists in `./pretrained/network.pt`. If not, it runs the training script first (training is kept in a separate file for better command line functionality). Please ensure that the network specifications in the two files is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./pretrained/network.pt'):\n",
    "    os.system('python train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kwsonsnn.model import ShowCaseNet\n",
    "from kwsonsnn.utils import get_default_net\n",
    "\n",
    "# Construct network\n",
    "network = get_default_net()\n",
    "\n",
    "# Load pre-trained network\n",
    "network.load_state_dict(torch.load('./pretrained/network.pt'))\n",
    "network.eval()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Old code snippets\n",
    "\n",
    "This section is not meant to be executed, but rather looked at for inspiration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE FOR A SINGLE PLOT\n",
    "#plt.figure(figsize=(7,4))\n",
    "#data = train_data[0]\n",
    "#audio, label = data['audio'], data['label']\n",
    "#sns.lineplot(x=np.arange(len(audio)), y=audio)\n",
    "#plt.xlim(0, len(audio))\n",
    "#plt.ylim(min(audio)*1.05, max(audio)*1.05)\n",
    "#plt.title(f'Keyword: {label}')\n",
    "#plt.xlabel('Sample')\n",
    "#plt.ylabel('Amplitude')\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE FOR A SINGLE PLOT\n",
    "#data = train_data[0]\n",
    "#frames, label = data['audio'], data['label']\n",
    "#plt.figure(figsize=(5,4))\n",
    "#ax = sns.heatmap(frames.T)\n",
    "#ax.invert_yaxis()\n",
    "#plt.xlabel('Time')\n",
    "#plt.ylabel('MFC Coefficients')\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}