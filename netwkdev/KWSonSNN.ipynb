{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Spotting on Spiking Neural Networks\n",
    "\n",
    "This is a test Jupyter notebook with implementations of various SNNs for keyword spotting (KWS) on SNNs. The networks developed will end up being implemented in an embedded hardware accelerator targeting low-power computing for e.g. hearing aids or similar. SNNs are ideal for such applications because of their sparse activity, and the application's relatively low requirements in terms of throughput. \n",
    "\n",
    "The notebook uses a combination of Numpy, PyTorch, and BindsNET libraries for this task. The dataset considered is a subset of relevant keywords from the speech commands dataset from Google, see [here](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html).\n",
    "\n",
    "The hardware accelerator is available in this repository under `./src/main/scala/neuroproc`. The accelerator is written in the Chisel language, which is an open-source HDL within Scala, see [here](https://github.com/chipsalliance/chisel3). Its design is based on [work by Anthon Riber](https://github.com/Thonner/NeuromorphicProcessor) and is developed as part of a master's thesis at the Institute of Mathematics and Computer Science, DTU Compute, Technical University of Denmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install bindsnet seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "from kwsonsnn.utils import download\n",
    "download('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First step in this process is loading and understanding the dataset. Additionally, the data may need various kinds of preprocessing to be used in the models (e.g. FFT or similar, see [this](https://towardsdatascience.com/speech-classification-using-neural-networks-the-basics-e5b08d6928b7)). The box above should have downloaded the dataset - granted it is not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kwsonsnn.dataset import SpeechCommandsDataset\n",
    "train_data = SpeechCommandsDataset('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some random non-preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "data = train_data[0]\n",
    "audio, label = data['audio'], data['label']\n",
    "sns.lineplot(x=np.arange(len(audio)), y=audio)\n",
    "plt.xlim(0, len(audio))\n",
    "plt.ylim(min(audio)*1.05, max(audio)*1.05)\n",
    "plt.title(f'Keyword: {label}')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us apply the preprocessing steps we wish to do and plot the resulting data. First of all, an FFT of the data will effectively transform the data to a plottable spectrogram. Secondly, the frequency spectrum data will be converted into Mel cepstral coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us plot some processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch some data\n",
    "data = train_data[0]\n",
    "frames, label = data['audio'], data['label']\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "ax = sns.heatmap(frames.T)\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MFC Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Encoding\n",
    "\n",
    "Next, we shall explore the encoding of the data fed into the network. Generally, we observe that power consumption is highly correlated with spike activity and thus, we should aim for as few spikes per time step as possible. The original work implements training and evaluation with rate-based encoding; direct rate encoding for simulation and training, and indirect rate-period encoding for efficient storage in the accelerator.\n",
    "\n",
    "This project, however, sets out to optimize this by using a timing-based encoding instead. This affects only the input to the network in that the activity in the subsequent layers is less dependent on data encoding.\n",
    "\n",
    "First, we encode the data with rate encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct rate encoding\n",
    "from kwsonsnn.encode import RateEncoder\n",
    "enc = RateEncoder(500)\n",
    "print(enc(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect rate encoding\n",
    "from kwsonsnn.encode import RatePeriod\n",
    "enc = RatePeriod(500)\n",
    "print(enc(frames))"
   ]
  },
  {
   "source": [
    "Notice how the periods calculated above are typically rather short relative to the 500 time steps of inputting spikes. This means that a massive amount of spike activity is seen in the input phase, which will likely not be reflected in subsequent layers.\n",
    "\n",
    "Let us instead consider rank-order encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct rank-order encoding\n",
    "# TODO: Fix this!\n",
    "from bindsnet.encoding.encoders import RankOrderEncoder\n",
    "enc = RankOrderEncoder(500)\n",
    "print(enc(frames))\n",
    "print(enc(frames).sum(dim=(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect rank-order encoding\n",
    "from kwsonsnn.encode import RankOrderPeriod\n",
    "enc = RankOrderPeriod(500)\n",
    "print(enc(frames))"
   ]
  },
  {
   "source": [
    "When using rank-order encoding, at most one input neuron spikes in each of the 500 time steps. This means that the total number of spikes generated by input spike trains is reduced by roughly an order of magnitude with significant power savings to follow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "The next step is to define a relevant model and prepare it for the supervised learning task that keyword spotting is. The following box checks that a pretrained model exists in `./pretrained/network.pt`. If not, it runs the training script first (training is kept in a separate file for better command line functionality). Please ensure that the network specifications in the two files is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./pretrained/network.pt'):\n",
    "    os.system('python train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kwsonsnn.model import ShowCaseNet\n",
    "from kwsonsnn.utils import get_default_net\n",
    "\n",
    "# Construct network\n",
    "network = get_default_net()\n",
    "\n",
    "# Load pre-trained network\n",
    "network.load_state_dict(torch.load('./pretrained/network.pt'))\n",
    "network.eval()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}